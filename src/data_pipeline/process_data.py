import pandas as pd
import pandas_ta as ta
from sklearn.preprocessing import StandardScaler
import os
import joblib
import warnings

# Suppress SettingWithCopyWarning, as we use .copy() where needed
pd.options.mode.chained_assignment = None
warnings.simplefilter(action='ignore', category=FutureWarning)

def process_data(raw_data_path: str, processed_data_path: str)-> pd.DataFrame: 
    """
    Loads raw stock data, calculates features, normalizes it, and saves the processed data.

    This function supports incremental updates. On the first run, it processes the entire
    raw data file, fits a StandardScaler, saves the processed data, and saves the scaler.
    On subsequent runs, it loads the existing processed data and the scaler, identifies
    new raw data for each ticker, processes only the new data, and appends it to the
    processed file.

    Args:
        raw_data_path (str): Path to the CSV file containing raw stock data.
        processed_data_path (str): Path to save the processed CSV file.

    Key Improvements:
    1.  **Lookahead Bias Fix**: All indicators and return features are shifted by 1 day.
    2.  **Incremental Updates**: Only processes new data for each ticker if a processed file
        already exists, using a 100-day buffer for accurate indicator calculation.
    3.  **Robust Normalization**: Uses a single `StandardScaler` fitted on the initial dataset
        and saved to disk (`scaler.joblib`) for consistent scaling on all subsequent runs.
    4.  **Dynamic Feature Detection**: Automatically detects columns generated by `pandas_ta`
        instead of relying on hardcoded names.
    5.  **Enhanced Logging**: Provides clear print statements for tracking progress.
    """
    if not os.path.exists(raw_data_path):
        print(f"Error: Raw data file not found at {raw_data_path}")
        return None

    # Define path for the scaler object, stored alongside the processed data
    scaler_path = os.path.join(os.path.dirname(processed_data_path), 'scaler.joblib') #need to save scaler in same directory as processed data
    os.makedirs(os.path.dirname(processed_data_path), exist_ok=True) 

    print("Loading raw data...")
    raw_df = pd.read_csv(raw_data_path, parse_dates=['Date'])

    # --- Data Validation & Cleaning ---
    # Convert core columns to numeric, coercing errors to NaN. This handles any non-numeric
    # values (e.g., error strings) that might have been saved in the raw CSV.
    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
        if col in raw_df.columns:
            raw_df[col] = pd.to_numeric(raw_df[col], errors='coerce')
    
    # Drop rows where essential data is missing (e.g., after coercion)
    raw_df.dropna(subset=['Close', 'Volume'], inplace=True)

    if raw_df.duplicated(subset=['Ticker', 'Date']).any():
        print("Warning: Found duplicate Ticker-Date rows. Keeping the last entry for each.")
        raw_df.drop_duplicates(subset=['Ticker', 'Date'], keep='last', inplace=True)

    if 'Ticker' not in raw_df.columns: #if ticker not in column, just end
        print("Error: 'Ticker' column not found in raw data.")
        return None

    # --- Determine Run Type: Full or Incremental ---
    is_incremental_run = os.path.exists(processed_data_path) and os.path.exists(scaler_path)
    last_dates = {}
    if is_incremental_run: #already has processed data.
        print("RUN TYPE: Incremental update.") 
        print("Loading existing processed data to determine last entry dates...")
        existing_df = pd.read_csv(processed_data_path, parse_dates=['Date'])
        # Get the last date for each ticker from the existing data
        last_dates = existing_df.groupby('Ticker')['Date'].max().to_dict()
    else: #no pre-existing data. 
        print("RUN TYPE: Full data processing.")
        if os.path.exists(processed_data_path):
             print(f"WARNING: Processed file '{processed_data_path}' exists but scaler is missing. Will overwrite.")
        elif os.path.exists(scaler_path):
             print(f"WARNING: Scaler file '{scaler_path}' exists but processed data is missing. Will overwrite.")
     #########################################################
     # --- Process Data Ticker by Ticker ---
    #########################################################
    all_new_data = []
    total_rows_added = 0
    # Store original columns to dynamically find indicator columns later
    original_cols = raw_df.columns.tolist()

    # --- Process Data Ticker by Ticker ---
    for ticker, group in raw_df.groupby('Ticker'):
        print(f"--- Processing ticker: {ticker} ---")

        last_date = last_dates.get(ticker) 
        if last_date:
            # For incremental runs, we need a buffer of old data to correctly calculate indicators
            # for the first few new data points (e.g., SMA_50 needs 50 days).
            buffer_days = 100
            buffer_start_date = last_date - pd.Timedelta(days=buffer_days)

            # Get the new data that needs to be processed
            new_data_to_process = group[group['Date'] > last_date]
            if new_data_to_process.empty:
                print(f"No new data for {ticker}. Skipping.")
                continue

            print(f"Found {len(new_data_to_process)} new rows for {ticker}.")
            # Get historical data to serve as a buffer for calculations
            historical_buffer = group[(group['Date'] <= last_date) & (group['Date'] >= buffer_start_date)]
            # The group we'll run calculations on includes the buffer and the new data
            calc_group = pd.concat([historical_buffer, new_data_to_process]).copy()
        else:
            # This is the first time we're seeing this ticker, or it's a full run
            print(f"Processing all {len(group)} rows for {ticker}.")
            new_data_to_process = group
            calc_group = group.copy()


        # --- Feature Engineering ---
        calc_group.set_index('Date', inplace=True)
        calc_group.sort_index(inplace=True) # Ensure data is sorted chronologically before calculations

        # Calculate technical indicators using pandas_ta
        calc_group.ta.rsi(length=14, append=True)
        calc_group.ta.sma(length=20, append=True)
        calc_group.ta.sma(length=50, append=True)
        calc_group.ta.ema(length=10, append=True)
        calc_group.ta.macd(append=True)
        calc_group.ta.atr(length=14, append=True)
        calc_group.ta.obv(append=True)

        # Calculate returns
        calc_group['return_1d'] = calc_group['Close'].pct_change(1)
        calc_group['return_5d'] = calc_group['Close'].pct_change(5)

        # --- FIX LOOKAHEAD BIAS ---
        # Dynamically identify all newly added columns (indicators and returns)
        feature_cols = [col for col in calc_group.columns if col not in original_cols]
        # Shift all calculated features by 1 to prevent using current-day info for prediction
        calc_group[feature_cols] = calc_group[feature_cols].shift(1)

        # --- Target Variable ---
        # y=1 if next day's close is higher, 0 otherwise. This is the label, so it uses future info.
        calc_group['y'] = (calc_group['Close'].shift(-1) > calc_group['Close']).astype(int)

        # --- Data Cleaning ---
        # Forward-fill handles NaNs from non-trading days or initial buffer period
        calc_group.ffill(inplace=True)
        # Drop any remaining NaNs (e.g., at the very start of the history or the last row for 'y')
        calc_group.dropna(inplace=True)

        # --- Finalize Processed Rows ---
        # Filter out the buffer rows, keeping only the new data we intended to process
        first_new_date = new_data_to_process['Date'].min()
        final_new_rows = calc_group[calc_group.index >= first_new_date].copy()
        final_new_rows.reset_index(inplace=True) # Move 'Date' from index to column

        if final_new_rows.empty:
            print(f"No usable new rows for {ticker} after cleaning. Skipping.")
            continue

        final_new_rows['Ticker'] = ticker # Re-add ticker column
        all_new_data.append(final_new_rows)
        rows_added = len(final_new_rows)
        total_rows_added += rows_added
        print(f"Added {rows_added} processed rows for {ticker}.")

    if not all_new_data:
        print("\nNo new data was processed across all tickers. Exiting.")
        return None

    # --- Combine, Normalize, and Save ---
    print(f"\nTotal new rows to process: {total_rows_added}")
    new_df = pd.concat(all_new_data, ignore_index=True)

    # Identify all numeric columns that should be scaled (OHLCV, features, returns)
    # Exclude identifiers ('Ticker', 'Date') and the target ('y')
    cols_to_scale = [
        col for col in new_df.columns
        if new_df[col].dtype in ['int64', 'float64'] and col not in ['Ticker', 'Date', 'y']
    ]
    
    # Ensure all columns to be scaled are present in the dataframe
    valid_cols_to_scale = [col for col in cols_to_scale if col in new_df.columns]

    if is_incremental_run:
        # On incremental runs, load the existing scaler and just transform the new data
        print("Loading existing scaler and transforming new data...")
        scaler = joblib.load(scaler_path)
        new_df[valid_cols_to_scale] = scaler.transform(new_df[valid_cols_to_scale])
        
        # Append the newly processed and scaled data to the existing file
        print(f"Appending {len(new_df)} new rows to {processed_data_path}...")
        new_df.to_csv(processed_data_path, mode='a', header=False, index=False)
    else:
        # On a full run, fit the scaler on the entire new dataset
        print("Fitting new scaler on the full dataset...")
        scaler = StandardScaler()
        new_df[valid_cols_to_scale] = scaler.fit_transform(new_df[valid_cols_to_scale])
        
        # Save the fitted scaler and the full processed dataframe
        print(f"Saving new scaler to {scaler_path}...")
        joblib.dump(scaler, scaler_path)
        print(f"Saving {len(new_df)} new rows to {processed_data_path}...")
        new_df.to_csv(processed_data_path, mode='w', header=True, index=False)

    print("\nProcessing complete.")
    return new_df
if __name__ == "__main__":
    process_data(raw_data_path="Data/raw_data.csv", processed_data_path="Data/processed_data.csv")